{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f8c2d12",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6a502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers torchvision torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94933702",
   "metadata": {},
   "source": [
    "## 1. & 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "617b7a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vikra\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.jpg: a dog is walking on a green carpet\n",
      "2.jpg: a small dog running across a green field\n",
      "3.jpg: the girls in the pool\n",
      "4.jpg: a bird perched on a plant\n",
      "5.jpg: a small dog standing on a stone ledge next to a pool\n",
      "6.jpg: a man riding a bike down the street\n",
      "7.jpg: a brown butterfly sitting on a green plant\n",
      "8.jpg: a man in a suit and tie sitting on a couch\n",
      "9.jpg: a duck drinking water from a pond\n",
      "10.jpg: a coffee machine with a cup of coffee\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Load model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "captions = {}\n",
    "\n",
    "# Loop images from 1 to 10 (Renamed)\n",
    "for i in range(1, 11):\n",
    "    image_path = f\"{image_dir}/{i}.jpg\"\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        # Preprocess\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**inputs)\n",
    "            caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        captions[f\"{i}.jpg\"] = caption\n",
    "        print(f\"{i}.jpg: {caption}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {i}.jpg: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6bb272",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd986b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.jpg: Similarity Score = 0.3008\n",
      "2.jpg: Similarity Score = 0.3295\n",
      "3.jpg: Similarity Score = 0.2696\n",
      "4.jpg: Similarity Score = 0.2800\n",
      "5.jpg: Similarity Score = 0.3225\n",
      "6.jpg: Similarity Score = 0.2648\n",
      "7.jpg: Similarity Score = 0.2965\n",
      "8.jpg: Similarity Score = 0.2876\n",
      "9.jpg: Similarity Score = 0.3037\n",
      "10.jpg: Similarity Score = 0.2816\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "\n",
    "# Load model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Hardcoded BLIPgenerated captions from previous cell\n",
    "captions = {\n",
    "    \"1.jpg\": \"a dog is walking on a green carpet\",\n",
    "    \"2.jpg\": \"a small dog running across a green field\",\n",
    "    \"3.jpg\": \"the girls in the pool\",\n",
    "    \"4.jpg\": \"a bird perched on a plant\",\n",
    "    \"5.jpg\": \"a small dog standing on a stone ledge next to a pool\",\n",
    "    \"6.jpg\": \"a man riding a bike down the street\",\n",
    "    \"7.jpg\": \"a brown butterfly sitting on a green plant\",\n",
    "    \"8.jpg\": \"a man in a suit and tie sitting on a couch\",\n",
    "    \"9.jpg\": \"a duck drinking water from a pond\",\n",
    "    \"10.jpg\": \"a coffee machine with a cup of coffee\"\n",
    "}\n",
    "\n",
    "# Image folder path\n",
    "image_dir = \"Images\"\n",
    "\n",
    "similarity_scores = {}\n",
    "\n",
    "# Loop through images (Renamed)\n",
    "for i in range(1, 11):\n",
    "    fname = f\"{i}.jpg\"\n",
    "    caption = captions[fname]\n",
    "    image_path = f\"{image_dir}/{fname}\"\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Process image\n",
    "    inputs = processor(text=[caption], images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    # Get CLIP embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        image_embeds = outputs.image_embeds\n",
    "        text_embeds = outputs.text_embeds\n",
    "\n",
    "    # Normalize embeddings\n",
    "    image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "    text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = (image_embeds @ text_embeds.T).squeeze().item()\n",
    "    similarity_scores[fname] = similarity\n",
    "\n",
    "    print(f\"{fname}: Similarity Score = {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd5479a",
   "metadata": {},
   "source": [
    "## 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a4b4427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikra\\AppData\\Local\\Temp\\ipykernel_29904\\74238980.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.jpg: Similarity Score = 0.1849\n",
      "2.jpg: Similarity Score = 0.2014\n",
      "3.jpg: Similarity Score = 0.1510\n",
      "4.jpg: Similarity Score = 0.1666\n",
      "5.jpg: Similarity Score = 0.1887\n",
      "6.jpg: Similarity Score = 0.1526\n",
      "7.jpg: Similarity Score = 0.1739\n",
      "8.jpg: Similarity Score = 0.1298\n",
      "9.jpg: Similarity Score = 0.1705\n",
      "10.jpg: Similarity Score = 0.1330\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import os\n",
    "from open_clip import create_model_from_pretrained, get_tokenizer\n",
    "\n",
    "# Load model and processor\n",
    "model, preprocess = create_model_from_pretrained('hf-hub:UCSC-VLAA/ViT-L-14-CLIPS-Recap-DataComp-1B')\n",
    "tokenizer = get_tokenizer('hf-hub:UCSC-VLAA/ViT-L-14-CLIPS-Recap-DataComp-1B')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# Hardcoded BLIPgenerated captions from previous cell\n",
    "captions = {\n",
    "    \"1.jpg\": \"a dog is walking on a green carpet\",\n",
    "    \"2.jpg\": \"a small dog running across a green field\",\n",
    "    \"3.jpg\": \"the girls in the pool\",\n",
    "    \"4.jpg\": \"a bird perched on a plant\",\n",
    "    \"5.jpg\": \"a small dog standing on a stone ledge next to a pool\",\n",
    "    \"6.jpg\": \"a man riding a bike down the street\",\n",
    "    \"7.jpg\": \"a brown butterfly sitting on a green plant\",\n",
    "    \"8.jpg\": \"a man in a suit and tie sitting on a couch\",\n",
    "    \"9.jpg\": \"a duck drinking water from a pond\",\n",
    "    \"10.jpg\": \"a coffee machine with a cup of coffee\"\n",
    "}\n",
    "\n",
    "# Image folder path\n",
    "image_dir = \"Images\"\n",
    "\n",
    "similarity_scores = {}\n",
    "\n",
    "# Loop through images (Renamed)\n",
    "for i in range(1, 11):\n",
    "    fname = f\"{i}.jpg\"\n",
    "    caption = captions[fname]\n",
    "    image_path = f\"{image_dir}/{fname}\"\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Tokenize caption\n",
    "    text = tokenizer([caption], context_length=model.context_length).to(device)\n",
    "\n",
    "    # Encode and compute similarity\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        image_features = model.encode_image(image_tensor)\n",
    "        text_features = model.encode_text(text)\n",
    "        image_features = F.normalize(image_features, dim=-1)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        similarity = (image_features @ text_features.T).squeeze().item()\n",
    "\n",
    "    similarity_scores[fname] = similarity\n",
    "\n",
    "    print(f\"{fname}: Similarity Score = {similarity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
