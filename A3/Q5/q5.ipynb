{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73836ec0",
   "metadata": {},
   "source": [
    "# Matcher: Segment Anything with One Shot Using Feature Matching\n",
    "\n",
    "This notebook evaluates the Matcher framework on image pairs from the Images folder. For each subfolder containing two images, we'll use each image as a reference for the other to generate segmentation masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293c2484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install the required packages for the Matcher framework\n",
    "# %pip install future==0.18.2 gradio==3.32.0 gradio-client==0.2.5 POT omegaconf iopath\n",
    "# # Additional important dependencies that may be needed\n",
    "# %pip install matplotlib torch torchvision opencv-python timm numpy tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e16709",
   "metadata": {},
   "source": [
    "## Setup and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f3b80a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "Added c:\\Users\\vikra\\OneDrive\\Desktop\\CSE344-CV\\A3\\Q5\\Matcher to sys.path\n",
      "Added c:\\Users\\vikra\\OneDrive\\Desktop\\CSE344-CV\\A3\\Q5\\Matcher\\segment_anything to sys.path\n",
      "Added c:\\Users\\vikra\\OneDrive\\Desktop\\CSE344-CV\\A3\\Q5\\Matcher\\utils to sys.path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Add Matcher directory to path\n",
    "matcher_path = os.path.join(os.getcwd(), \"Matcher\")\n",
    "if matcher_path not in sys.path:\n",
    "    sys.path.append(matcher_path)\n",
    "    print(f\"Added {matcher_path} to sys.path\")\n",
    "    \n",
    "# Add segment_anything to path\n",
    "sam_path = os.path.join(os.getcwd(), \"Matcher\", \"segment_anything\")\n",
    "if sam_path not in sys.path:\n",
    "    sys.path.append(sam_path)\n",
    "    print(f\"Added {sam_path} to sys.path\")\n",
    "\n",
    "# Add utils to path\n",
    "utils_path = os.path.join(os.getcwd(), \"Matcher\", \"utils\")\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.append(utils_path)\n",
    "    print(f\"Added {utils_path} to sys.path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2143493",
   "metadata": {},
   "source": [
    "## Download and Setup Models\n",
    "\n",
    "We'll load the necessary foundation models used by the Matcher framework: DINOv2 and SAM (Segment Anything Model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6df17373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DINOv2 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\vikra/.cache\\torch\\hub\\facebookresearch_dinov2_main\n",
      "C:\\Users\\vikra/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "C:\\Users\\vikra/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "C:\\Users\\vikra/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n",
      "C:\\Users\\vikra/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "C:\\Users\\vikra/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "C:\\Users\\vikra/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n",
      "C:\\Users\\vikra/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "C:\\Users\\vikra/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "C:\\Users\\vikra/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINOv2 model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load DINOv2 model\n",
    "print(\"Loading DINOv2 model...\")\n",
    "dinov2_vitl14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
    "dinov2_vitl14 = dinov2_vitl14.eval()\n",
    "if torch.cuda.is_available():\n",
    "    dinov2_vitl14 = dinov2_vitl14.cuda()\n",
    "print(\"DINOv2 model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ce5bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAM checkpoint already exists.\n"
     ]
    }
   ],
   "source": [
    "# Download SAM model if not already available\n",
    "import requests\n",
    "\n",
    "# Create directory for SAM checkpoint\n",
    "sam_checkpoint_dir = \"sam_checkpoints\"\n",
    "os.makedirs(sam_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Download SAM model (vit_h)\n",
    "sam_url = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n",
    "sam_path = os.path.join(sam_checkpoint_dir, \"sam_vit_h_4b8939.pth\")\n",
    "\n",
    "if not os.path.exists(sam_path):\n",
    "    print(f\"Downloading SAM checkpoint to {sam_path}...\")\n",
    "    with requests.get(sam_url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(sam_path, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    print(\"SAM checkpoint downloaded.\")\n",
    "else:\n",
    "    print(\"SAM checkpoint already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429fa8ee",
   "metadata": {},
   "source": [
    "## Import Matcher Components\n",
    "\n",
    "Now we'll import the necessary components from the Matcher framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61324925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9bc8f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff1be05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported modules from Matcher!\n"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "# Import SAM (Segment Anything Model)\n",
    "from segment_anything.build_sam import sam_model_registry\n",
    "from segment_anything.predictor import SamPredictor\n",
    "\n",
    "# Import from Matcher repo\n",
    "from matcher.Matcher import Matcher\n",
    "\n",
    "# For DINOv2 image preprocessing\n",
    "from torchvision import transforms\n",
    "\n",
    "print(\"Successfully imported modules from Matcher!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8914d22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def read_image(path):\n",
    "    return cv2.imread(path)\n",
    "\n",
    "def image_resize(image, height=None, width=None):\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    if height is not None:\n",
    "        ratio = height / float(h)\n",
    "        new_w = int(w * ratio)\n",
    "        new_h = height\n",
    "    elif width is not None:\n",
    "        ratio = width / float(w)\n",
    "        new_w = width\n",
    "        new_h = int(h * ratio)\n",
    "    else:\n",
    "        return image\n",
    "\n",
    "    # Make sure dimensions are divisible by 14\n",
    "    new_h -= new_h % 14\n",
    "    new_w -= new_w % 14\n",
    "\n",
    "    return cv2.resize(image, (new_w, new_h))\n",
    "\n",
    "\n",
    "\n",
    "def ensure_same_size(img1, img2):\n",
    "    h, w = min(img1.shape[0], img2.shape[0]), min(img1.shape[1], img2.shape[1])\n",
    "    return cv2.resize(img1, (w, h)), cv2.resize(img2, (w, h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2bb5a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\vikra/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing SAM model...\n",
      "SAM model initialized!\n",
      "\n",
      "Initializing Feature Matching model...\n",
      "Feature Matching model initialized!\n",
      "SAM model initialized!\n",
      "\n",
      "Initializing Feature Matching model...\n",
      "Feature Matching model initialized!\n"
     ]
    }
   ],
   "source": [
    "# Function to initialize SAM model\n",
    "def get_sam_model(checkpoint, model_type=\"vit_h\"):\n",
    "    \"\"\"Initialize and return a SAM model.\"\"\"\n",
    "    sam = sam_model_registry[model_type](checkpoint=checkpoint)\n",
    "    if torch.cuda.is_available():\n",
    "        sam.to(device=\"cuda\")\n",
    "    return sam\n",
    "\n",
    "# Function to get DINOv2 feature maps\n",
    "def get_dino_feat_maps(dino_model, dino_img):\n",
    "    \"\"\"Extract and return feature maps from DINOv2.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        features = dino_model.forward_features(dino_img)\n",
    "        features_dict = {k: v for k, v in features.items()}\n",
    "    return features_dict[\"x_norm_patchtokens\"]\n",
    "\n",
    "# Function to adapt image for DINOv2 processing\n",
    "def adapt_img_to_dino(img):\n",
    "    \"\"\"Preprocess an image for the DINOv2 model.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "    \n",
    "    # Convert to PIL if it's numpy array\n",
    "    if isinstance(img, np.ndarray):\n",
    "        img = Image.fromarray(img.astype('uint8'))\n",
    "        \n",
    "    tensor = transform(img).unsqueeze(0)\n",
    "    return tensor\n",
    "\n",
    "dinov2_vitl14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
    "\n",
    "# Function to get the Matcher model\n",
    "def get_feature_match_model():\n",
    "    \"\"\"Initialize and return the Matcher model.\"\"\"\n",
    "    score_filter_cfg = {\n",
    "        \"score_thresh\": 0.05,\n",
    "        \"iou_thresh\": 0.7,\n",
    "        \"min_area\": 100\n",
    "    }\n",
    "    return Matcher(encoder=dinov2_vitl14, score_filter_cfg=score_filter_cfg)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "print(\"Initializing SAM model...\")\n",
    "sam = get_sam_model(checkpoint=sam_path)\n",
    "print(\"SAM model initialized!\")\n",
    "\n",
    "print(\"\\nInitializing Feature Matching model...\")\n",
    "matching_model = get_feature_match_model()\n",
    "print(\"Feature Matching model initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf919d5a",
   "metadata": {},
   "source": [
    "## Define Helper Functions\n",
    "\n",
    "Let's define helper functions to perform one-shot segmentation using the Matcher framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aba124f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_one_shot_segmentation(ref_image_path, src_image_path):\n",
    "    \"\"\"Perform one-shot segmentation from ref → src using SAM and Matcher.\"\"\"\n",
    "    # Read images\n",
    "    ref_image = read_image(ref_image_path)\n",
    "    src_image = read_image(src_image_path)\n",
    "\n",
    "\n",
    "    required_size = matching_model.input_size[-1]  # e.g., 518\n",
    "    ref_image = image_resize(ref_image, height=required_size)\n",
    "    src_image = image_resize(src_image, height=required_size)\n",
    "\n",
    "\n",
    "    # Ensure same size\n",
    "    ref_image, src_image = ensure_same_size(ref_image, src_image)\n",
    "\n",
    "    # Prepare image for DINOv2\n",
    "    preprocess_dino = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # Ref + Src as tensors for DINO\n",
    "    ref_dino_img = preprocess_dino(ref_image).unsqueeze(0)\n",
    "    src_dino_img = preprocess_dino(src_image).unsqueeze(0)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        ref_dino_img = ref_dino_img.cuda()\n",
    "        src_dino_img = src_dino_img.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ref_feat_map = get_dino_feat_maps(dinov2_vitl14, ref_dino_img)\n",
    "        src_feat_map = get_dino_feat_maps(dinov2_vitl14, src_dino_img)\n",
    "\n",
    "    # -------- Reference Setup -------- #\n",
    "    ref_tensor = ref_dino_img  # Already normalized and batched\n",
    "\n",
    "    # Create binary mask at center of image\n",
    "    h, w = ref_image.shape[:2]\n",
    "    center_y, center_x = h // 2, w // 2\n",
    "    mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    mask[center_y, center_x] = 1  # One-hot point mask\n",
    "\n",
    "    mask_tensor = torch.from_numpy(mask).unsqueeze(0).unsqueeze(0).float()\n",
    "    if torch.cuda.is_available():\n",
    "        mask_tensor = mask_tensor.cuda()\n",
    "\n",
    "    # Set reference and target\n",
    "    matching_model.set_reference(ref_tensor, mask_tensor)\n",
    "    matching_model.set_target(src_feat_map)  # ✅ Only feature map here\n",
    "    all_sam_masks = matching_model.predict(sam=sam)\n",
    "\n",
    "    # Return results\n",
    "    return {\n",
    "        \"reference_image\": ref_image,\n",
    "        \"source_image\": src_image,\n",
    "        \"masks\": all_sam_masks\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd7f7eb",
   "metadata": {},
   "source": [
    "## Process All Image Pairs\n",
    "\n",
    "Now let's process all image pairs in the Images folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "024cc7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 subfolders in the Images directory:\n",
      "['backpack', 'backpack_dog', 'barn', 'bear_plushie', 'berry_bowl', 'can', 'candle', 'cat', 'cat2', 'cat_statue', 'chair', 'clock', 'colorful_sneaker', 'colorful_teapot', 'dog', 'dog2', 'dog3']\n"
     ]
    }
   ],
   "source": [
    "# Get list of all subfolders in the Images directory\n",
    "images_dir = \"Images\"\n",
    "subfolders = [f for f in os.listdir(images_dir) if os.path.isdir(os.path.join(images_dir, f))]\n",
    "print(f\"Found {len(subfolders)} subfolders in the Images directory:\")\n",
    "print(subfolders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c4d671f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single subfolder\n",
    "def process_subfolder(subfolder):\n",
    "    print(f\"\\nProcessing subfolder: {subfolder}\")\n",
    "    subfolder_path = os.path.join(images_dir, subfolder)\n",
    "    image_files = [f for f in os.listdir(subfolder_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    if len(image_files) < 2:\n",
    "        print(f\"Skipping {subfolder} - insufficient images\")\n",
    "        return\n",
    "    \n",
    "    # For this demo, we'll use just the first two images\n",
    "    img1_path = os.path.join(subfolder_path, image_files[0])\n",
    "    img2_path = os.path.join(subfolder_path, image_files[1])\n",
    "    \n",
    "    print(f\"Image pair: {image_files[0]} and {image_files[1]}\")\n",
    "    \n",
    "    # First direction: Use img1 as reference, img2 as source\n",
    "    print(f\"\\nDirection 1: {image_files[0]} (ref) → {image_files[1]} (src)\")\n",
    "    results1 = perform_one_shot_segmentation(img1_path, img2_path)\n",
    "    visualize_segmentation_results(results1, img1_path, img2_path)\n",
    "    \n",
    "    # Second direction: Use img2 as reference, img1 as source\n",
    "    print(f\"\\nDirection 2: {image_files[1]} (ref) → {image_files[0]} (src)\")\n",
    "    results2 = perform_one_shot_segmentation(img2_path, img1_path)\n",
    "    visualize_segmentation_results(results2, img2_path, img1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c07fe502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3 selected subfolders: ['backpack', 'backpack_dog', 'barn']\n",
      "\n",
      "Processing subfolder: backpack\n",
      "Image pair: 00.jpg and 05.jpg\n",
      "\n",
      "Direction 1: 00.jpg (ref) → 05.jpg (src)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'predictor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(selected_subfolders)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m selected subfolders: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselected_subfolders\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m subfolder \u001b[38;5;129;01min\u001b[39;00m selected_subfolders:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[43mprocess_subfolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mprocess_subfolder\u001b[39m\u001b[34m(subfolder)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# First direction: Use img1 as reference, img2 as source\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDirection 1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_files[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (ref) → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_files[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (src)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m results1 = \u001b[43mperform_one_shot_segmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg1_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg2_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m visualize_segmentation_results(results1, img1_path, img2_path)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Second direction: Use img2 as reference, img1 as source\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mperform_one_shot_segmentation\u001b[39m\u001b[34m(ref_image_path, src_image_path)\u001b[39m\n\u001b[32m     45\u001b[39m     mask_tensor = mask_tensor.cuda()\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Set reference and target\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[43mmatching_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_reference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m matching_model.set_target(src_feat_map)  \u001b[38;5;66;03m# ✅ Only feature map here\u001b[39;00m\n\u001b[32m     50\u001b[39m all_sam_masks = matching_model.predict(sam=sam)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vikra\\OneDrive\\Desktop\\CSE344-CV\\A3\\Q5\\Matcher\\matcher\\Matcher.py:102\u001b[39m, in \u001b[36mMatcher.set_reference\u001b[39m\u001b[34m(self, imgs, masks)\u001b[39m\n\u001b[32m    100\u001b[39m ref_masks_pool = F.avg_pool2d(masks, (\u001b[38;5;28mself\u001b[39m.encoder.patch_size, \u001b[38;5;28mself\u001b[39m.encoder.patch_size))\n\u001b[32m    101\u001b[39m nshot = ref_masks_pool.shape[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m ref_masks_pool = (ref_masks_pool > \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m.model.mask_threshold).float()\n\u001b[32m    103\u001b[39m ref_masks_pool = ref_masks_pool.reshape(-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# nshot, N\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[38;5;28mself\u001b[39m.ref_imgs = imgs\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'predictor'"
     ]
    }
   ],
   "source": [
    "# Process a few selected subfolders (you can change this to process all)\n",
    "selected_subfolders = subfolders[:3]  # Start with first 3 for demonstration\n",
    "print(f\"Processing {len(selected_subfolders)} selected subfolders: {selected_subfolders}\")\n",
    "\n",
    "for subfolder in selected_subfolders:\n",
    "    process_subfolder(subfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e3bceb",
   "metadata": {},
   "source": [
    "# CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa86449e",
   "metadata": {},
   "source": [
    "## Process All Remaining Subfolders\n",
    "\n",
    "Run this cell to process all remaining subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b112184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process remaining subfolders\n",
    "remaining_subfolders = subfolders[3:]\n",
    "print(f\"Processing {len(remaining_subfolders)} remaining subfolders: {remaining_subfolders}\")\n",
    "\n",
    "for subfolder in remaining_subfolders:\n",
    "    process_subfolder(subfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e4e93",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we demonstrated the use of the Matcher framework for one-shot segmentation. For each pair of images, we performed segmentation in both directions (using each image as the reference for the other). The results show how well the framework can transfer segmentation from a reference image to a target image using feature matching between DINOv2 features, followed by mask generation with the Segment Anything Model (SAM)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
